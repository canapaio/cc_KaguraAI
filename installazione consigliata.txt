#HW consigliato
NVida >=3070 8GB min
32GB RAM
CPU 12/16 core

# Fasi
1.1 Installare ubuntu server 22.04 su sistema x86/64, docker e compose
    https://www.server-world.info/en/note?os=Ubuntu_22.04&p=install
  1.2 Docker
    Rimuovi il vecchio docker consigliato # sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras

    Installazione:
    https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository

    # Add Docker's official GPG key:
    sudo apt-get update
    sudo apt-get install ca-certificates curl
    sudo install -m 0755 -d /etc/apt/keyrings
    sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
    sudo chmod a+r /etc/apt/keyrings/docker.asc

    # Add the repository to Apt sources:
    echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    sudo apt-get update

    sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
  
  1.3 Docker Compose:
    in lavoraizone     
  
2.1 Installare driver nvidia, varie e CUDA


3 installare ollama
  https://www.server-world.info/en/note?os=Ubuntu_22.04&p=ollama&f=1
  4.1 assicurarsi di modificare ollama.service aggiunendo in [Service] Environment="OLLAMA_HOST=0.0.0.0"

3.1 configurare ollama
  Devi fare pull del modello che vuoi usare
  ollama pull mistral
  ollama pull gemma2


3.2 vivamente consigliato crearsi il propio modello scaricandolo da Hugginface e compialndo un file ad oc per le tue esigenze
  ollama create nome_modello -f file_configurazione_modello.file

4 Docer Cuda
sudo apt update && sudo apt upgrade nvidia-container-toolkit 
4.0 ## Installlare docker da repository
4.1 ##
https://linuxconfig.org/setting-up-nvidia-cuda-toolkit-in-a-docker-container-on-debian-ubuntu
4.2 ##
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey -o /tmp/nvidia-gpgkey


6 installare CCat con qdrant
  usa pure il mio compese.yml sembra funzionare :)
    https://raw.githubusercontent.com/canapaio/cc_KaguraAI/main/compose.yml

7 configura CCat con il modello che hai scelto http://ip_ccat:1865/admin/settings (ip_ccat pu√≤ essere localhost o l'ip della macchina remota dove riposa il gatto)
    7.1 Conf. L.Model 
        ollama
    7.2 Base Url 
        http://ip_home_ollama_serve:11434
    7.3 Model:
        uno di quelli inseriti con ollama pull modello, o quello che ti sei creato (Mistral, Gemma2 o nome_modello)

8 ti consiglio di usre matrioska embedder e sbert, mi pare funzioni bene
    https://github.com/nickprock/matryoshka-embeddings
    https://github.com/nickprock/sbert-integration

prima di iniziare a lavorare seriamente fai tante prove e installa tutti i plugin che ritieni opportuni

i plugin possono cancellare la memoria del gatto, prima di fare un massiccio inserimento di dati assicurati di aver testato e selezionato i plugin giusti
